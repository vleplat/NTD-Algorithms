%% Review template

\documentclass[a4paper, 11pt]{article}


% Input encoding
\usepackage[utf8]{inputenc}

% Graphics packages
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usetikzlibrary{shapes,arrows}
\usepackage[margin=0.5in]{geometry}


% Math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{array}
%\usepackage{mathabx} % Big times
%\usepackage[boxruled]{algorithm2e}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathrsfs}

% Others
\usepackage{cite}
\usepackage{enumerate}
\usepackage{url}
\usepackage{lipsum}
\usepackage{booktabs} % for \toprule

% Font, symbols and color packages
\usepackage{dsfont}
\usepackage{latexsym}
\usepackage{hhline}
\usepackage{color}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{bbding} % for Checkmark

\input{notations.tex}
%%------------------------ Title page ------------------------------%%

\title{Multiplicative updates for Nonnegative Tucker problems}

\author{Valentin Leplat, Jeremy E. Cohen, ??}
\date{}

%%--------------------- End of Title page --------------------------%%

\begin{document}
\maketitle

\section{Some notes on sparsity and normalization}

Consider the toy problem
\begin{equation}\label{eq:toypb}
  \underset{X\in\mathbb{R}^{m\times r},Y\in\mathbb{R}^{r\times n}}{\argmin}f(XY) + \mu_x \|X\|_1 + \frac{1}{2}\mu_y \|Y\|_F^2
\end{equation}
where \( f \) is some cost function, and \( \mu_{x,y} \) are nonnegative regularization parameters. We can be more general than this using arbitrary regularizations, but let's keep it simple for now. Let us denote \( \phi(X,Y) \) the cost function, and we surcharge the notation with \( \phi(X,Y,\mu_x,\mu_y) \) abusively when hyperparameters may vary.

We make a few observations:
\begin{itemize}
  \item If \( \mu_y =0\), then because \( f(XY) \) is invariant to a scaling of columns (resp. rows) of \( X \) (resp. \( Y \)), we have for any couple \( X,Y \) and any \( \lambda<1 \)
  \begin{equation}
      \phi(\lambda X, \frac{1}{\lambda} Y) = f(XY) + \mu_x \lambda \|X\|_1 < \phi(X,Y)~.
  \end{equation}
  This means that the cost can always be decreased by scaling down the columns of \( X \), which implies that a global solution must be \( X=0 \) which is however impossible. In other words this problem is degenerate and does not admit good NMF solutions, let alone sparse solutions. In practice the regularization term is will grow extremely small and the problem becomes simply \( \argmin_{X,Y} f(XY) \).
  \item Because of the scaling ambiguity, it was shown in~\cite{}[Roald 2021 todo] that we can fix \( \mu_x = \mu_y \) without loss of generality. Indeed, for any positive $u$,
  \begin{equation}
    \phi(X,Y,\mu,\mu) = f(XY) + \mu \|X\|_1 + \frac{\mu}{2} \|Y\|_F^2 = f(\frac{X}{u} uY) + \mu u  \|\frac{X}{u} \|_1 + \frac{\mu}{2u^2} \|uY\|_F^2 = \phi(\tilde{X},\tilde{Y}, \mu u, \frac{\mu}{u^2})
  \end{equation}
  where \( \tilde{X} \) and \( \tilde{Y} \) are new, scaled variable. We then see that
  \begin{equation}\label{eq:equiv-reg}
    \underset{X,Y}{~\min~} \phi(X,Y,\mu,\mu) = \underset{X,Y}{~\min~} \phi(\tilde{X},\tilde{Y},\mu u,\frac{\mu}{u^2}) = \underset{\tilde{X},\tilde{Y}}{~\min~} \phi(\tilde{X},\tilde{Y},\mu u,\frac{\mu}{u^2}) =  \underset{X,Y}{~\min~} \phi(X,Y,\mu u,\frac{\mu}{u^2})~.
  \end{equation}
  Because any couple \( (\mu_x,\mu_y) \) can be written as \( (\mu u, \frac{\mu}{u^2}) \), setting \( \mu_x = \mu_y = \mu \) does not change the minimum of the cost (but we do change the argmin). Alternatively, we can fix one of the two regularization parameters arbitrarily.
  \item When \( \mu_x = \mu_y = \mu \), it can be shown that the solution \((X^\ast,Y^\ast)\) must satisfy \( \|X^\ast\|_1 = \|Y^\ast\|_F^2 \). Indeed, fix \(a=\mu\|X^\ast\|_1, b=\mu\|Y^\ast\|_F^2\), and minimize the one-dimensional cost
  \( \phi(\lambda):= f(\frac{X}{\lambda} \lambda Y) + a \lambda + \frac{b}{\lambda^2}\) for positive lambda. It can be shown easily that \( \lambda^\ast = (\frac{b}{a})^{1/3} \). However, if \( \lambda^\ast \) is not equal to one, it means that \( \phi(X,Y) \) can be reduced by scaling and thus \( X^\ast \) and \( Y^\ast \) are not solutions. By contraposition we must have \( a=b \) which yields
  \( \|X^\ast\|_1 = \|Y^\ast\|_F^2 \). This is very interesting because it means that we can balance the terms in the decomposition using penalisations on each term. In fact using the same reasoning with columnwise scaling ambiguity yields the stronger result \( \|X^\ast_i\|_1 = \|Y^\ast_i\|_2^2 \) for any column/row index \( i \). This also means that we should in fact not use \( \mu_x = \mu_y \) because we will not be able to decrease the norm of \( X \) without also decreasing \( Y \) which may heavily biais the fitting term \( f(XY) \). Rather, a sound strategy is to fix \(\mu_y = 1 \) and only tune \( \mu_x \). We will then have at optimality \( \mu_x\|X^\ast_i\|_1 = \|Y^\ast_i\|_2^2  \) which allows to scale the \( \ell_1 \) norm of columns of \( X \) as desired.
\end{itemize}

\bibliographystyle{plain}
\bibliography{all_refs}


\end{document}
