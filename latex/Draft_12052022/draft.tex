%% Review template

\documentclass[a4paper, 11pt]{article}


% Input encoding
\usepackage[utf8]{inputenc}

% Graphics packages
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usetikzlibrary{shapes,arrows}
\usepackage[margin=0.5in]{geometry}


% Math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{array}
%\usepackage{mathabx} % Big times
%\usepackage[boxruled]{algorithm2e}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathrsfs}

% Others
\usepackage{cite}
\usepackage{enumerate}
\usepackage{url}
\usepackage{lipsum}
\usepackage{booktabs} % for \toprule

% Font, symbols and color packages
\usepackage{dsfont}
\usepackage{latexsym}
\usepackage{hhline}
\usepackage{color}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{bbding} % for Checkmark

\input{notations.tex}
%%------------------------ Title page ------------------------------%%

\title{Algorithms for Nonnegative Tucker problems}

\author{Valentin Leplat, Jeremy E. Cohen, ??}
\date{}

%%--------------------- End of Title page --------------------------%%

\begin{document}
\maketitle

\section{Paper content}
- intro
- parcimonie
- beta= 2 HALS
- optimi dure


todo:
- codes synthé $>$ Valentin
- examples $>$ jérémy

\section{Litterature on NTD}

\subsection{Model properties}
\begin{itemize}
    \item Zhou Cichocki 2014 Uniqueness 
    \item Cohen 2017, Skau 2022 Compression (nnCANDELIND)
    \item (nice paper!, first paper on full nonnegative NTD and algorithms) Morup 2008 Sparse NTD (MU) + interpretability
    \item Semi-supervised NTD (graph regularized, label propagation) (Qiu 2021, Li 2017)
    \item 
\end{itemize}

\subsection{Applications}

\begin{itemize}
    \item 3D skeleton classification (Li 2021)
    \item Brain source separation (Morup 2008)
    \item Music factorization and segmentation (Smith 2019, Marmoret 2020)
    \item Blind Unmixing (Sun 2021), HSI superresolution (Zare, Kazemi 2021), HSI compression (Li 2017)
    \item Neuroimaging EEG (Phan 2010, Dao 2018, Rostakova 2020)
    \item patient Phenotyping (Yang 2019)
\end{itemize}

\subsection{Algorithms from NMF and sparse/orthogonal NMF}
\begin{itemize}
    \item Paatero 1998
    \item Lee and Song 1999, first MU, 2000 proof convergence
    \item Choi 2008 Orthogonal NMF
    \item Fevotte xxx (majorante jointe + papier avec Jerome)
    \item Gillis xxx (2011 accHALS, 2012 ``sparse and unique'' sparse HALS with max=1 normalization, 2018 HER HALS)
    \item (help Valentin!)
    Sparse things
    \begin{itemize}
        \item Normalization (LeRoux  xx )
        \item Mixed sparsity objective (Hoyer 2002 2004)
        \item l1 et max=1 (Gillis 2012 sparse and unique)
        \item 
    \end{itemize}
\end{itemize}

\subsection{Algorithms for NTD and CPD}
\begin{itemize}
    \item Xu Yin 2016 APG
    \item ANLS HALS (Cichocki 2011) $>$ better version with nicolas' HALS but not paper on it $>$ cover beta=2 more specifically as well ! Already used in nn-fac toolbox but never explained.
    \item Sparse TD parallel Kaya, Ucar 2015
    \item Tucker Sketching Tropp 2019
    \item Zdunek 2011 and Junjun Pan 2021 Orthogonal NTD
    \item (NTD !!) Kim Choi 2007 (also below)
    \item Incremental (Online) NTD Zdunek 2022
\end{itemize}

\subsection{Non-euclidean losses in Tensor Decompositions}
\begin{itemize}
    \item Hong, Kolda GCP
    \item Vandecapelle, Vervliet 2019, 2020
    \item (NTD !!) Kim Choi 2007
    \item Pu, Xiao Fu 2021 stochastic mirror descent
    \item Ghalamkari, Sugiyama 2021 Mean-Field (not nonnegative)
\end{itemize}

\section{Some insights on sparsity and normalization}

Consider the toy problem
\begin{equation}\label{eq:toypb}
  \underset{X\in\mathbb{R}^{m\times r},Y\in\mathbb{R}^{r\times n}}{\argmin}f(XY) + \mu_x \|X\|_1 + \frac{1}{2}\mu_y \|Y\|_F^2
\end{equation}
where \( f \) is some cost function, and \( \mu_{x,y} \) are nonnegative regularization parameters. We can be more general than this using arbitrary regularizations, but let's keep it simple for now. Let us denote \( \phi(X,Y) \) the cost function, and we surcharge the notation with \( \phi(X,Y,\mu_x,\mu_y) \) abusively when hyperparameters may vary.

We make a few observations:
\begin{itemize}
  \item If \( \mu_y =0\), then because \( f(XY) \) is invariant to a scaling of columns (resp. rows) of \( X \) (resp. \( Y \)), we have for any couple \( X,Y \) and any \( \lambda<1 \)
  \begin{equation}
      \phi(\lambda X, \frac{1}{\lambda} Y) = f(XY) + \mu_x \lambda \|X\|_1 < \phi(X,Y)~.
  \end{equation}
  This means that the cost can always be decreased by scaling down the columns of \( X \), which implies that a global solution must be \( X=0 \) which is however impossible. In other words this problem is degenerate and does not admit good NMF solutions, let alone sparse solutions. In practice the regularization term is will grow extremely small and the problem becomes simply \( \argmin_{X,Y} f(XY) \).
  \item Because of the scaling ambiguity, it was shown in~\cite{}[Roald 2021 todo] that we can fix \( \mu_x = \mu_y \) without loss of generality. Indeed, for any positive $u$,
  \begin{equation}
    \phi(X,Y,\mu,\mu) = f(XY) + \mu \|X\|_1 + \frac{\mu}{2} \|Y\|_F^2 = f(\frac{X}{u} uY) + \mu u  \|\frac{X}{u} \|_1 + \frac{\mu}{2u^2} \|uY\|_F^2 = \phi(\tilde{X},\tilde{Y}, \mu u, \frac{\mu}{u^2})
  \end{equation}
  where \( \tilde{X} \) and \( \tilde{Y} \) are new, scaled variable. We then see that
  \begin{equation}\label{eq:equiv-reg}
    \underset{X,Y}{~\min~} \phi(X,Y,\mu,\mu) = \underset{X,Y}{~\min~} \phi(\tilde{X},\tilde{Y},\mu u,\frac{\mu}{u^2}) = \underset{\tilde{X},\tilde{Y}}{~\min~} \phi(\tilde{X},\tilde{Y},\mu u,\frac{\mu}{u^2}) =  \underset{X,Y}{~\min~} \phi(X,Y,\mu u,\frac{\mu}{u^2})~.
  \end{equation}
  Because any couple \( (\mu_x,\mu_y) \) can be written as \( (\mu u, \frac{\mu}{u^2}) \), setting \( \mu_x = \mu_y = \mu \) does not change the minimum of the cost (but we do change the argmin). Alternatively, we can fix one of the two regularization parameters arbitrarily.
  \item When \( \mu_x = \mu_y = \mu \), it can be shown that the solution \((X^\ast,Y^\ast)\) must satisfy \( \|X^\ast\|_1 = \|Y^\ast\|_F^2 \). Indeed, fix \(a=\mu\|X^\ast\|_1, b=\mu\|Y^\ast\|_F^2\), and minimize the one-dimensional cost
  \( \phi(\lambda):= f(\frac{X}{\lambda} \lambda Y) + a \lambda + \frac{b}{\lambda^2}\) for positive lambda. It can be shown easily that \( \lambda^\ast = (\frac{b}{a})^{1/3} \). However, if \( \lambda^\ast \) is not equal to one, it means that \( \phi(X,Y) \) can be reduced by scaling and thus \( X^\ast \) and \( Y^\ast \) are not solutions. By contraposition we must have \( a=b \) which yields
  \( \|X^\ast\|_1 = \|Y^\ast\|_F^2 \). This is very interesting because it means that we can balance the terms in the decomposition using penalisations on each term. In fact using the same reasoning with columnwise scaling ambiguity yields the stronger result \( \|X^\ast_i\|_1 = \|Y^\ast_i\|_2^2 \) for any column/row index \( i \). This also means that we should in fact not use \( \mu_x = \mu_y \) because we will not be able to decrease the norm of \( X \) without also decreasing \( Y \) which may heavily biais the fitting term \( f(XY) \). Rather, a sound strategy is to fix \(\mu_y = 1 \) and only tune \( \mu_x \). We will then have at optimality \( \mu_x\|X^\ast_i\|_1 = \|Y^\ast_i\|_2^2  \) which allows to scale the \( \ell_1 \) norm of columns of \( X \) as desired.
\end{itemize}

\section{Algorithms for Sparse beta-divergence NTD}
In this section we propose various algorithms able to compute a candidate solution to approximate Nonnegative Tucker Decomposition (NTD) with $\beta$-divergence as a loss function and mixed-sparsity regularizations on the factors and the core tensor:

\begin{equation}\label{eq:genoptiprob}
  \underset{W \geq 0,H \geq 0,Q \geq 0,\mathcal{G} \geq 0}{\argmin}D_{\beta}(\mathcal{X}|\mathcal{G} \times_1 W \times_2 H \times_3 Q) + \mu_{\mathcal{G}} \|\mathcal{G}\|_1 + \frac{1}{2} \left( \mu_W \|W\|_F^2 + \mu_H \|H\|_F^2 + \mu_Q \|Q\|_F^2 \right)
\end{equation}
with $D_{\beta}(.|.)$ the element-wise $\beta$-divergence between two tensors, and $\mu_{\mathcal{G}}$, $\mu_W$, $\mu_H$ and $\mu_Q$ are positive scalars denoting the penalty weights associated to each sparsity regularization. For the later, those will be further referred to as penalty functions.

The objective function in \eqref{eq:genoptiprob} is non-convex jointly in $\{W,H,Q,\mathcal{G}\}$. Moreover, computing a global solution to NTD is NP-Hard since NTD generalizes NMF \cite{Vavasis2010complexity,marmoret2021nonnegative}. Hence most algorithms developed to solve NMF and therefore NTD optimization problems are based on iterative local optimization schemes converging to local solutions. For such optimization problems, it is usually easier to optimize over one factor (or one mode for NTD) given the others terms are known and fixed. Indeed, the obtained subproblems when fixing all but one mode is convex as long as $\beta \in \left[1,2\right]$. For this reason many of the algorithms developed to tackle \eqref{eq:genoptiprob} or its NMF variants rely on Block Coordinate Descent (BCD) schemes and we adopt this strategy in this paper. 



Furthermore, this work extends previous works that have been carried out to tackle NTD with $\beta$-divergence as loss function. Indeed, the seminal paper by Lee and Seung \cite{Lee1999Learning} proposed the first alternating algorithm for NMF with $\beta$-divergence but without penalty functions. These algorithms have been later revisited by \cite{fevotte2011algorithms} and \cite{doi:10.1137/20M1377278} (this one integrates penalty functions and equality constraints) and finally extended to non-penalized $\beta$-divergence NTD in \cite{marmoret2021nonnegative}. One of the core ideas in \cite{marmoret2021nonnegative} is the fact that the NTD model can be rewritten using tensor matricization along the different modes. For instance, along the first mode we have:
\begin{equation}\label{eq:matriciTensor}
    \begin{aligned}
        & \mathcal{X} = \mathcal{G} \times_1 W \times_2 H \times_3 Q \\
        & \Leftrightarrow \mathcal{X}_{(1)} = W \mathcal{G}_{(1)} \left( H \kron Q \right)^T
    \end{aligned}
\end{equation}
where $\mathcal{X}_{(i)}$ is the matricization of the tensor $\mathcal{X}$ along the mode $i$ and $\kron$ denotes the Kronecker product. The matricizations are analogous for factors $H$ and $Q$. Hence, Equation \eqref{eq:matriciTensor} can be interpreted as an NMF of $\mathcal{X}_{(i)}$ with factors $W$ and $\mathcal{G}_{(1)} \left( H \kron Q \right)^T$. This observation led the authors in \cite{marmoret2021nonnegative} to develop Multiplicative Updates based algorithms to derive the updates of each factor of NTD with $\beta$-divergence loss function. In this paper, we follow similar approaches.  


The two next sections respectively present the updates for the factors $W,H$ and $Q$ and the core tensor $\mathcal{G}$ to tackle \eqref{eq:genoptiprob}.

\subsection{Updates of the factors}\label{subsec_upfac}
In this section we derive updates for factors $W,H,Q$. Since all the subproblems have similar structure, we will only focus on the subproblem in $W$ without loss of generality. 

For clarity, let us start by considering the matricization of the tensor $\mathcal{X}$ along the first mode given by Equation \eqref{eq:matriciTensor} and pose $V:=\mathcal{X}_{(1)} \in \mathbb{R}_+^{F \times N}$ and $U:=\mathcal{G}_{(1)} \left( H \kron Q \right)^T \in \mathbb{R}_+^{K \times N}$. Note that $\mathcal{G}_{(1)} \left( H \kron Q \right)^T$ can be computed based on the following identity: $\mathcal{G}_{(1)} \left( H \kron Q \right)^T=\left(\mathcal{G} \times_2 H \times_3 Q \right)_{(1)}$.
The subproblem in $W$ is defined as follows:

\begin{equation}\label{eq:probinW}
  \underset{W \geq 0}{\argmin}D_{\beta}(V|WU) + \frac{1}{2} \mu_W \|W\|_F^2
\end{equation}
The objective function in Problem \eqref{eq:probinW} is separable with respect to the rows of $W$, that is $D_{\beta}(V|WU) + \frac{1}{2} \mu_W \|W\|_F^2=\sum_f^F \left[D_{\beta}(v_f|w_f U) + \frac{1}{2} \mu_W \|w_f\|_F^2 \right]$, hence can be minimized over the $F$ rows of $W$ independently. For the following, we focus on the minimization over one particular row $f$ of $W$ leading to the following optimization problem:
\begin{equation}\label{eq:probinw}
  \underset{w \geq 0}{\argmin}D_{\beta}(v|wU) + \frac{1}{2} \mu_W \|w\|_F^2
\end{equation}
where the selected subscript $f$ has been dropped for clarity purposes and $D_{\beta}(v|wU)=\sum_{n}d_{\beta}(v_{n}|\left[wU\right]_{n})$ with the discrete $\beta$-divergence denoted $d_{\beta}(x|y)$ and equal to 
\[
%d_{\beta}(x|y) = 
\left\{
  \begin{array}{lr}
    \frac{1}{\beta \left(\beta-1\right)}   \left(x^{\beta}+\left(\beta-1\right)y^{\beta}-\beta xy^{\beta-1}\right)  \text{ for } 
     \beta \neq 0,1, \\
    x\log\frac{x}{y}-x+y           
         \text{ for } \beta=1,  \\
    \frac{x}{y}-\log\frac{x}{y}-1  
        \text{ for } \beta=0. 
  \end{array}
\right.
\] 
For $\beta=2$, this the standard squared Euclidean distance, that is, the squared Frobenius norm $||V-WU||_F^2$. 
For $\beta=1$ and $\beta=0$, 
the $\beta$-divergence corresponds to the Kullback-Leibler (KL) divergence and the Itakura-Saito (IS) divergence, respectively. 


In order to tackle Problem \eqref{eq:probinw}, we follow the Majorization-Minimization (MM) framework. Let us briefly recall the high-level ideas to derive updates via MM.
Let us consider the general problem
\[
\min_{w \in \mathcal{W}} f(w). 
\] 
Given an initial iterate $\widetilde{w}\in \mathcal{W}$, MM generates a new iterate $\hat{w}\in \mathcal{W}$ that is guaranteed to decrease the objective function, that is,
$f\big( \hat{w} \big) \leq f\big( \widetilde{w}  \big)$. To do so, it uses the following two steps: 
\begin{itemize} 
\item Majorization: find a function that is an upper approximation of the objective and is tight at the current iterate, which is referred to as a majorizer. More precisely find a function $g\big(w|\widetilde{w}\big)$ such that 
\[
(i) ~ g\big(\widetilde{w}|\widetilde{w}\big) = f\big(\widetilde{w}\big) 
\quad \text{ and } \quad 
(ii)  ~  g\big({w}|\widetilde{w}\big) \geq f({w}) \text{ for all } w \in  \mathcal{W}. 
\]

\item Minimization: minimize the majorizer, that is, 
solve $\min_{w \in \mathcal{W}} g\big({w}|\widetilde{w}\big)$ approximately or exactly, to obtain the next iterate $\hat{w} \in \mathcal{W}$ which is such that $(iii) ~ g\big(\hat{w}|\widetilde{w}\big)  \leq g(\widetilde{w}|\widetilde{w})$. 
This guarantees the objective function to decrease at each step of this iterative process since 
\[
f\big(\hat{w}\big) 
\; 
\underset{(ii)}{\leq} 
\; 
g\big(\hat{w}|\widetilde{w}\big) 
\; 
\underset{(iii)}{\leq}  
\; 
g\big(\widetilde{w}|\widetilde{w}\big) 
\; 
\underset{(i)}{=} 
\; 
f\big(\widetilde{w}\big). 
\] 
\end{itemize}  
The updates are obtained using MM where the majorizer $g$ is chosen separable, that is, $g\big(w|\widetilde{w}\big) = \sum_{i=1} g_i\big(w_i|\widetilde{w}_i\big)$ for some well chosen univariate functions $g_i$'s; discussed later in the next section. 
This choice typically makes the minimization of $g$ admit a closed-form solution.

In Problem \eqref{eq:probinw}, the second term is separable w.r.t. each entry of $w$.
For the data fitting term, we use the majorizer proposed in \cite{fevotte2011algorithms}. For the sake of completeness, we briefly recall it in the following. It consists in majorizing the convex part of the $\beta$-divergence using Jensen's inequality and majorizing the concave part by its tangent (first-order Taylor approximation). We have 
  \begin{equation}\label{eq:3}
  d_{\beta}(x|y)= \check{d}_{\beta}(x|y)+\hat{d}_{\beta}(x|y)+\bar{d}_{\beta}(x|y), 
  \end{equation}
where $\check{d}$ is convex function of $y$, $\hat{d}$ is a concave function of $y$ and $\bar{d}$ is a constant of $y$; see Table~\ref{table:conv_concav_decomp}.  
\begin{center}
\begin{table}[h!]
\begin{center}
\caption{Differentiable convex-concave-constant decomposition of the $\beta$-divergence under the form \eqref{eq:3}~\cite{fevotte2011algorithms}. }
\label{table:conv_concav_decomp}
\begin{tabular}{|c|c|c|c|}
\hline 
      & $\check{d}(x|y)$  &  $\hat{d}(x|y)$ & $\bar{d}(x)$   \\  \hline 
 $\beta=0$      
 & $xy^{-1}$     & $\log(y)$  & $x(\log(x)-1)$  \\
 $\beta \in [1,2] $ 
 & $d_{\beta}(x|y)$ & 0 & 0    \\ \hline  
\end{tabular} 
\end{center}
\end{table}
\end{center}


\begin{lemma}[\cite{fevotte2011algorithms}] \label{defG}
Let $\tilde{v} = \tilde{w}U$ and $\tilde{w}$ be such that $\tilde{v_{n}} >0$ for all $n$ and $\tilde{w_{k}} >0$ for all $k$.  
Then the function 
\begin{equation}\label{eq:20}
\begin{aligned}
G(w|\tilde{w})&=\sum_{n}\left[\sum_{k} \frac{\tilde{w_{k}}u_{kn}}{\tilde{v_{n}}}\check{d}(v_{n}|\tilde{v_{n}}\frac{w_{k}}{\tilde{w_{k}}})\right] +\bar{d}(v_{n})  \\
&+\left[\hat{d}^{'}(v_{n}|\tilde{v_{n}})\sum_{k}(w_{k}-\tilde{w_{k}})u_{kn}+\hat{d}(v_{n}|\tilde{v_{n}}) \right]
\end{aligned}
\end{equation} 
is a majorizer for $\sum_{n}d(v_{n}|\left[wU\right]_{n})$ at $\tilde{w}$.
\end{lemma}
Finally the problem we need to solve has the form:
\begin{equation}\label{eq:probinwupper}
  \underset{w \geq 0}{\argmin}G(w|\tilde{w}) + \frac{1}{2} \mu_W \|w\|_F^2
\end{equation}
Optimization problem \eqref{eq:probinwupper} is a particular instance of problems covered by framework proposed in \cite{doi:10.1137/20M1377278} in the case no additional equality constraints are required. More formally, assuming $\mu_W > 0$,  Problem \eqref{eq:probinwupper} satisfies:
\begin{itemize}
    \item Assumption 1 from \cite{doi:10.1137/20M1377278} since the penalty function $\Phi(w)=\frac{1}{2}\|w\|_F^2$ is lower bounded on the feasible set and admits a particular upper approximation at any current iterate $\widetilde{w}$:
    \begin{equation}
        \Phi(w) \leq \Phi(\widetilde{w}) + \langle \nabla\Phi(\widetilde{w}),w-\widetilde{w} \rangle + \sum_k \frac{L_k}{2}(w_k-\widetilde{w}_k)^2
    \end{equation}
    assuming $L_k \geq 1 > 0$ for all $k$.
    \item first setting covered by Proposition 1 \cite{doi:10.1137/20M1377278}, namely coefficients $a_k=\mu_W \frac{L_k}{2}$ are strictly positive for all $k$.
\end{itemize}

Therefore, by Proposition 1 from \cite{doi:10.1137/20M1377278}, there exists a unique minimizer of \eqref{eq:probinwupper} in $(0,\infty)$. In the following, we illustrate this theoretical result by computing the closed form expression of this minimizer. Note that the closed form expression can be derived if $\beta \in \{0,1,3/2,2\}$ according to \cite{doi:10.1137/20M1377278}.  Outside this set of values for $\beta$, an iterative scheme such as Newton-Raphson is required to compute the minimizer. Indeed, we will see further that the task of computing the minimizer is equivalent of finding the positive real roots of a set of monovariate polynomial equations in each entry $w_k$ of $w$.

To compute the positive minimizer of \eqref{eq:probinwupper}, we are looking for $w \in \mathbb{R}^{K}$ that cancels the gradient of objective function $G(w|\tilde{w}) + \frac{1}{2} \mu_W \|w\|_F^2$. Since the objective function is separable w.r.t. each entry $w_k$, we focus on solving:
\begin{equation}\label{eq:gradTocancel}
    \text{find } \hat{w_k} \text{ such that } \nabla_{w_k}\left[ G(w|\tilde{w}) + \frac{1}{2} \mu_W \|w\|_F^2 \right] = 0
\end{equation}

As mentioned previously, the next steps depend on the value chosen for $\beta$. In the following, we consider the particular cases $\beta \in \{1,3/2,2\}$ for which a closed form of the minimizer $\hat{w}$ can be derived, with a particular emphasis on the case $\beta = 1$.
Similar approach can be followed for the case $\beta =0 $ and is detailed in Appendix \ref{AppendA}.

Based on Lemma \ref{defG} and Table~\ref{table:conv_concav_decomp} for $\beta \in [1,2] $, we have:
\begin{equation}
    \begin{aligned}
        \nabla_{w_k}G(w|\tilde{w})& =\sum_n \frac{\tilde{w_{k}}u_{kn}}{\tilde{v_{n}}}  \nabla_{w_k} d_{\beta}(v_{n}|\tilde{v_{n}}\frac{w_{k}}{\tilde{w_{k}}})\\
        & = \sum_n u_{kn} \left[ \left( \tilde{v_{n}}\frac{w_{k}}{\tilde{w_{k}}} \right)^{\beta-1} - v_n \left( \tilde{v_{n}}\frac{w_{k}}{\tilde{w_{k}}} \right)^{\beta-2} \right]
    \end{aligned}
\end{equation}
since $\nabla_y d_{\beta}(x|y)=y^{\beta-1}-xy^{\beta-2}$. Equation \eqref{eq:gradTocancel} becomes:
\begin{equation}\label{eq:polybeta}
    \begin{aligned}
       & \nabla_{w_k}\left[ G(w|\tilde{w}) + \frac{1}{2} \mu_W \|w\|_F^2 \right] = 0\\
        & \Leftrightarrow a w_k + b w_k^{\beta-1} - c w_k^{\beta-2} = 0 \\
        & \Leftrightarrow a w_k^{3-\beta} + b w_k - c  = 0 \text{ since } \hat{w_k} \in (0,\infty).
    \end{aligned}
\end{equation}
where:
\begin{itemize}
    \item $a=\mu_W > 0$ by hypothesis,
    \item $b=\sum_n u_{kn} \left( \frac{\tilde{v_{n}}}{\tilde{w_{k}}} \right)^{\beta-1}  \geq 0$ given that $\tilde{v_{n}}$, $\tilde{w_{k}}$ and $u_{kn}$ are nonnegative for all $n$ and $k$.
    \item $c=\sum_n u_{kn} v_n \left( \frac{\tilde{v_{n}}}{\tilde{w_{k}}} \right)^{\beta-2} \geq 0$ given that $v_{n}$, $\tilde{v_{n}}$, $\tilde{w_{k}}$ and $u_{kn}$ are nonnegative for all $n$ and $k$.
\end{itemize}
Therefore, computing the $k$-th entry of the minimizer $\hat{w}$ is equivalent to finding the root of a monovariate polynomial equation of degree $3-\beta$ (for $\beta \in [1,2] $ ) in $w_k$. Further, we focus on the particular case $\beta=1$. 

For $\beta=1$, Equation \eqref{eq:polybeta} becomes $a w_k^{2} + b w_k - c  = 0$. The positive (real) root is computed as follows:
\begin{equation}\label{eq:updatewkentry}
    \begin{aligned}
       \hat{w_{k}}=\frac{\sqrt{(\sum_n u_{kn})^2+4\mu_W\tilde{w_{k}}\sum_n u_{kn}\frac{v_n}{\tilde{v_{n}}}}-\sum_n u_{kn}}{2 \mu_W}
    \end{aligned}
\end{equation}
with $\mu_W > 0$.
Note that although the closed-form expression in Equation \eqref{eq:updatewkentry} has a negative term in the numerator of the right-hand side, it can be easily checked that it always remains nonnegative given $v_n$, $u_{kn}$ and $\tilde{w_{k}}$ are nonnegative for all $k,n$. Equation \eqref{eq:updatewkentry} can be expressed in matrix form as follows:

\begin{equation}\label{eq:updateW}
    \begin{aligned}
       \hat{W}=\frac{\left[C^{.2}+S\right]^{.\frac{1}{2}}-C}{2 \mu_W}
    \end{aligned}
\end{equation}

where $C=eU^T$ with $e$ is a all-one matrix of size $F$-by-$N$ and $S=4\mu_W \tilde{W} \odot \left( \frac{\left[ V \right]}{\left[ \tilde{W}U \right]} U^T\right) $ with $A \odot B$ (resp. $\frac{\left[ A \right]}{\left[ B \right]}$ ) is the Hadamard product (resp. division) between $A$ and $B$ and $A^{(.\alpha)}$ is the element-wise $\alpha$ exponent of $A$.

Some important insights are discussed here-under:
\begin{itemize}
    \item Computing the limit $\lim_{\mu_W \to 0} \hat{W}(\mu_W)$ makes Equation \eqref{eq:updateW} tends to the original Multiplicative Updates introduced by Lee and Seung \cite{Lee1999Learning}. Indeed let us compute this limit in the scalar case from Equation \eqref{eq:updatewkentry} and pose $\alpha=\sum_n u_{kn}$ and $\eta=\tilde{w_{k}}\sum_n u_{kn}\frac{v_n}{\tilde{v_{n}}}$ for convenience:
    \begin{equation}
        \begin{aligned}
           \lim_{\mu_W \to 0} \frac{\sqrt{\alpha^2+4\mu_W \eta}-\alpha}{2 \mu_W} & = \frac{"0"}{"0"} \\
           & \underset{\text{H}}{=}\lim_{\mu_W \to 0} \frac{\frac{\partial}{\partial_{\mu_W}} \sqrt{\alpha^2+4\mu_W \eta}-\alpha}{\frac{\partial}{\partial_{\mu_W}}2 \mu_W}\\
           & = \lim_{\mu_W \to 0} (\alpha^2+4\mu_W\eta)^{-\frac{1}{2}} \eta = \frac{\eta}{\alpha}=\tilde{w_{k}}  \frac{\sum_n u_{kn}\frac{v_n}{\tilde{v_{n}}}}{\sum_n u_{kn}}
        \end{aligned}
    \end{equation}
    In matrix form we have:
    \begin{equation}
        \begin{aligned}
           \lim_{\mu_W \to 0} \hat{W}(\mu_W) = \tilde{W} \odot \frac{\left[ \frac{\left[ V \right]}{\left[ \tilde{W}U \right]} U^T \right]}{\left[ eU^T \right]}
        \end{aligned}
    \end{equation}
    which are the MU proposed by Lee and Seung for $\beta=1$.
    \item Regarding the analysis of monovariate polynomial equation given in \eqref{eq:polybeta}, interestingly the existence of a unique positive real root could have been also established by using Descartes rules of sign. Indeed we can count the number of real positive roots that $p(w_k)=a w_k^{3-\beta} + b w_k - c$ has (for $\beta \in [1,2] $). More specifically, let $v$ be the number of variations in the sign of the coefficients $a,b,c$ (ignoring coefficients that are zero). Let $n_p$ be the number of real positive roots. Then:
    \begin{enumerate}
        \item $n_p \leq v$,
        \item $v-n_p$ is an even integer.
    \end{enumerate}
     Let us consider the case $\beta=1$ as an example: given the polynomial $p(w_k)=a w_k^{2} + b w_k - c$, assuming that $c$ is positive. Then $v=1$, so $n_p$ is either 0 or 1 by rule 1. But by rule 2, $v-n_p$ must be even, hence $n_p=1$. Similar conclusion can be made for any $\beta \in [1,2] $.

\end{itemize}

As mentioned earlier, similar rationale can be followed to derive updates in closed-form for $\beta=3/2$ or $\beta=2$. 
The derivation of the updates in the case $\beta = 0$ is detailed in Appendix A since the polynomial equation differs from the one given in Equation \eqref{eq:polybeta}.
For other values of $\beta$, a numerical scheme will be necessary to compute the real positive root of the obtained polynomial equations. We can cite the Newton's method, the Muller's method and the the procedure developed in \cite{rootpoly} which is based on the explicit calculation of the intermediary root of a canonical form of cubic. This procedure is suited for providing highly accurate numerical results in the case of badly conditioned polynomials.


In the next section we detail the updates for the core tensor $\mathcal{G}$.

\subsection{Update of the core tensor}
For the core tensor, we start by using the vectorization property defined as follows:
\begin{equation}
    \begin{aligned}
       \text{vec}\left(\mathcal{X}\right)=\left(W \kron H \kron Q \right) \text{vec}\left(\mathcal{G}\right)
    \end{aligned}
\end{equation}
The updates for the core tensor follows the approach detailed in Section \ref{subsec_upfac} for factors. Let us pose $U:=\left(W \kron H \kron Q \right) \in \mathbb{R}^{F \times K}$, $v:=\text{vec}\left(\mathcal{X}\right) \in \mathbb{R}^{F}$  and $g:=\text{vec}\left(\mathcal{G}\right) \in \mathbb{R}^{K}$. The subproblem in $g$ is defined as follows:
\begin{equation}\label{eq:probinG}
  \underset{g \geq 0}{\argmin}D_{\beta}(v|Ug) + \mu_{\mathcal{G}} \|g\|_1
\end{equation}
Again we follow the MM framework, the final problem we need to solve has the form:
\begin{equation}\label{eq:probinGupper}
  \underset{g \geq 0}{\argmin}G(g|\tilde{g}) + \mu_{\mathcal{G}} \|g\|_1
\end{equation}

where $\|g\|_1=\sum_k g_k$ since $g \geq 0$. Again Problem \eqref{eq:probinGupper} is a particular instance of problems covered by framework proposed in [3] in the case no additional equality constraints are required. According to Proposition 1 from \cite{doi:10.1137/20M1377278}, there exists a unique minimizer of \eqref{eq:probinGupper} in $(0,\infty)$ as soon as $\beta < 2$. Indeed, we will illustrate later that a positive real minimizer is not guaranteed to exist when $\beta \geq 2$. Moreover, according to \cite{doi:10.1137/20M1377278}, it is possible to derive closed-form expressions for the minimizer of Problem \eqref{eq:probinGupper} for the following values of $\beta$: $\beta \in (-\infty,1] \cup \{5/4,4/3,3/2\}$. Outside these values for $\beta$, an iterative scheme is required to numerically compute the minimizer. To be compliant with the developments presented in Section \ref{subsec_upfac}, we will consider the interval $\beta \in [1,2)$ and more particularly the case $\beta=1$. For this setting, we can show that computing the minimizer of Problem \eqref{eq:probinGupper} corresponds to solving:

\begin{equation}\label{eq:polybetah}
    \begin{aligned}
       & \nabla_{g_k}\left[ G(g|\tilde{g}) + \mu_{\mathcal{G}} \|g\|_1 \right] = 0\\
        & \Leftrightarrow a h_k^{2-\beta} + b h_k - c  = 0 \text{ since } \hat{g_k} \in (0,\infty).
    \end{aligned}
\end{equation}
where:
\begin{itemize}
    \item $a=\mu_{\mathcal{G}} > 0 $ by hypothesis,
    \item $b=\sum_f u_{fk} \left( \frac{\tilde{v_{f}}}{\tilde{g_{k}}} \right)^{\beta-1} \geq 0$ given $u_{fk}$, $\tilde{v_{f}}=[Ug]_f$ and $\tilde{g_{k}}$ nonnegative for all $f,k$.
    \item $c=\sum_f u_{fk} v_f \left( \frac{\tilde{v_{f}}}{\tilde{g_{k}}} \right)^{\beta-2} \geq 0$ given $u_{fk}$, $v_{f}$, $\tilde{v_{f}}$ and $\tilde{g_{k}}$ nonnegative for all $f,k$.
\end{itemize}

For $\beta=1$, Equation \eqref{eq:polybetah} becomes $a g_k + b g_k - c  = 0$. The positive (real) root is computed as follows:
\begin{equation}\label{eq:updatehkentry}
    \begin{aligned}
       \hat{g_{k}} & =\frac{\left.c\right|_{\beta=1}}{a+\left.b\right|_{\beta=1}} \\
                   & = \tilde{g_{k}}\frac{\sum_f u_{fk}   \frac{v_f}{\tilde{v_{f}}} }{\mu_{\mathcal{G}}+\sum_f u_{fk}}
    \end{aligned}
\end{equation}
which is nonnegative since $\mu_{\mathcal{G}} > 0$ and given $u_{fk}$, $v_{f}$ ,$\tilde{v_{f}}$ and $\tilde{g_{k}}$ nonnegative for all $f,k$.  Equation \eqref{eq:updatehkentry} can be expressed in matrix form as follows:
\begin{equation}\label{eq:updateH}
    \begin{aligned}
       \hat{g}&=\tilde{g} \odot \frac{[U^T \frac{[ v ]}{[Ug]}]}{[\mu_{\mathcal{G}}e_K+U^Te_F]}
    \end{aligned}
\end{equation}

where $e_K$ and $e_F$ are all-ones column vectors of appropriate size.


Some important insights are discussed here-under:
\begin{itemize}
    \item Case $\beta=2$: as discussed earlier, the existence of a unique minimizer for Problem \eqref{eq:probinGupper} on $(0,\infty)$ is guaranteed for $\beta < 2$. Let us illustrate this theoretical bound by considering the case $\beta=2$. For such value Equation \eqref{eq:polybetah} becomes: $a+bg_k-c=0$, hence leading to the following expression for the minimizer $\hat{g_{k}}$: $\hat{g_{k}}=\frac{\left.c\right|_{\beta=2}-a}{\left.b\right|_{\beta=2}}$. Since there is a negative term in the numerator, there is no guarantee anymore that $\hat{g_{k}} \in (0,\infty)$ for any nonnegative $c$ and positive $a$.
    \item Other values of $\beta$: similar rationale can be followed to derive updates in closed-form for $\beta \in (-\infty,1) \cup \{5/4,4/3,3/2\}$. The derivation of the updates for the core tensor in the case $\beta = 0$ is detailed in Appendix A.
    \item Complexity and numerical costs: as opposed to factor updates, in most practical cases we cannot compute and store the matrix $U$ explicitly since it is much larger than the dataset itself. To tackle this issue, we follow the approach proposed in \cite{marmoret2021nonnegative} that is all products $Ut$ for any $t:=\text{vec}\left(\mathcal{T}\right)$ are computed using the following identity:
    \begin{equation}
        (W \kron H \kron Q)t=\text{vec}\left( \mathcal{T} \times_1 W \times_2 H \times_3 Q \right)
    \end{equation}
    Note that the products $U^Tt$ are computed similarly since the transposition is distributive with respect to the Kronecker product.
\end{itemize}
Algorithm~\ref{SparseKLNTD} summarizes our method to tackle \eqref{eq:genoptiprob} for the $\beta$-divergences in the particular case detailed above for $\beta=1$ which we refer to as Sparse KL-NTD algorithm. The updates for factors $H$ and $Q$ can be derived in the same way, by considering matricization \eqref{eq:matriciTensor} along modes 2 and 3 of the problem.

\algsetup{indent=2em}
\begin{algorithm}[ht!]
\caption{Sparse KL-NTD \label{SparseKLNTD}}
\begin{algorithmic}[1] 
\REQUIRE A tensor $\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times I_3}$, 
an initialization $\mathcal{G} \in \mathbb{R}^{K_1 \times K_2 \times K_3}_+$, $W \in \mathbb{R}^{I_1 \times K_1}_+$, $H \in \mathbb{R}^{I_2 \times K_2}_+$ and $Q  \in \mathbb{R}^{I_3 \times K_3}$, 
the factorization ranks $(K_1,K_2,K_3)$, a maximum number of iterations, maxiter, and weight vectors $\mu_{\mathcal{G}}>0$, $\mu_{W}>0$, $\mu_{H}>0$, $\mu_{Q}>0$.

\ENSURE A multirank-$(K_1,K_2,K_3)$ NTD $(\mathcal{G},W,H,Q)$ of $\mathcal{X}$.
    \medskip  
\FOR{$it$ = 1 : maxiter}
    \STATE \emph{\% Update of factor $W$} 
    \STATE $U \leftarrow\left(\mathcal{G} \times_2 H \times_3 Q \right)_{(1)}$
    \STATE $V \leftarrow \mathcal{X}_{(1)}$
    \STATE $C \leftarrow eU^T$
    \STATE $S \leftarrow 4\mu_W W \odot \left( \frac{\left[ V \right]}{\left[ WU \right]} U^T\right) $
    \STATE $W \leftarrow \frac{\left[C^{.2}+S\right]^{.\frac{1}{2}}-C}{2 \mu_W}$
    \STATE $H$ and $Q$ are updated in a similar way as $W$. 
	\STATE \emph{\% Update of core tensor $\mathcal{G}$}
    \STATE $\mathcal{N} \leftarrow \left( \mathcal{G} \times_1 W \times_2 H \times_3 Q \right)^{.(-1)} \odot \mathcal{X}$
    \STATE $\mathcal{G} \leftarrow \frac{[\mathcal{N} \times_1 W^T \times_2 H^T \times_3 Q^T]}{[\mu_{\mathcal{G}} \mathcal{E} + \mathcal{E} \times_1 W^T \times_2 H^T \times_3 Q^T]}$ with $\mathcal{E}$ an all-ones tensor of dimension $K_1 \times K_2 \times K_3$
\ENDFOR
\RETURN $\mathcal{G}$, $W$, $H$ and $Q$
\end{algorithmic}  
\end{algorithm} 

\textit{Computational cost}. The computational cost of Algorithm~\ref{SparseKLNTD} is asymptotically....

\bibliographystyle{plain}
\bibliography{all_refs}

\section*{Appendix A: Factor and core tensor updates for $\beta=0$}\label{AppendA}

\end{document}
